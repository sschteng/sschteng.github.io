---
layout: post
title: 笔记-吴恩达deeplearning之四
comments: true
date: 2017-09-19 17:30:18
tags:
- 笔记
- 深度学习
---
机器学习的应用是一个高度依赖经验的过程，
伴随着大量迭代的过程，需要训练诸多模型，才能找到适合的那一个，
所以优化算法能够帮助你快速训练模型。
使用好用的优化算法能够大大提高你的团队的效率，
<!--more-->
# mini-batch梯度下降法

众所周知，向量化能够有效地对所有m个例子进行计算，允许处理整个训练集，而无需某个明确的公式，所以我们要把训练样本放到巨大的矩阵X中去，$X=[x^{(1)}x^{(2)}x^{(3)}…x^{(m)}]，(n_x,m)$
Y也是如此，$Y=[y^{(1)}y^{(2)}y^{(3)}…y^{(m)}]，(1,m)$

但如果m很大的话，处理速度仍然缓慢，在对整个训练集执行梯度下降法时，
你必须处理整个训练集，才能进行一步梯度下降法，
然后你需要再重新处理，才能进行下一步梯度下降法，
所以你在处理完整个样本的训练集之前，先让梯度下降法处理一部分，你的算法速度会更快。

你可以把训练集X分割为小一点的子训练集，这些子集被取名为Mini-batch，以X^{t}表示
Y也应相应的拆分，以Y^{t}表示,Mini-batch的数量t组成了X^{t}和Y^{t}

运行Mini-batch梯度下降法时，
for t=1,...,5000(假设共有5000训练集)
$z^{[1]}=W^{[1]}X{{t}}+b^{[1]}$
$A^{[1]}=g^{[1]}(z^{[1]})$
$…$
$A^{[L]}=g^{[L]}(z^{[L]})$

J^{t}=$\frac{1}{1000}\sum_{i=1}^L l(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2*1000} \sum \|\|W^{[L]}\|\|_F^2$，(假设子集规模1000,$\hat{y}^{(i)},y^{(i)}$皆来自于子集)
之后对J^{t}做反向传播及梯度下降，
并循环上述步骤，知道收敛到一个合适的精度。

![](/assets/images/170919_1.JPG)
使用batch梯度下降法时，每次迭代都需要遍历整个训练集，可预期每次迭代的成本都会下降，
所以如果成本函数J是迭代次数的一个函数，它会随着每次迭代而减少，
使用Mini-batch梯度下降法时，如果你做出成本函数在整个过程中的图，
并不是每次迭代都是下降的，走向朝下，但会有很多噪声。

噪声原因也许是因为子集比较容易计算时，成本会低一些，较难计算时，成本会高一些，所以才会出现这些摆动。

需要决定的变量之一是Mini-batch的大小，m是训练集的大小，
极端情况下，Mini-batch的打小等于m，其实就是batch梯度下降法，
另一个极端情况，假设Mini-batch的大小为1，就有了新的算法，叫随机梯度下降法

Mini-batch大小的选择指导原则：
首先，如果训练集较小(少于2000个样本)，直接使用batch梯度下降法
不然，数目较大的话，一般的Mini-batch大小为64到512，基于电脑使用，如果大小是2的次方，代码会运行快一些
最后，Mini-batch中，X^{t}和Y^{t}要符合CPU/GPU内存，取决于你的应用方向以及训练集大小

# 指数加权平均

还有一些方法比梯度下降法速度快，要理解这些算法，需要用到指数加权平均，在统计中也叫做指数加权移动平均
![](/assets/images/170919_2.JPG)
例如温度统计，统计全年温度变化情况，
夏季温度转暖，然后冬季降温，用数据作图，

如果计算趋势的话，也就是温度的局部平均值，要做的是先使$V_0=0$，每天用0.9的加权数乘之前的数值，再加上当日温度
$v_1=0.9v_0+0.1\theta _1$
$v_2=0.9v_1+0.1\theta _2$
$…$
依此类推

![](/assets/images/170919_3.JPG)
权值越大，波动越小，曲线约平坦，因为会多平均几天温度
缺点是曲线会进一步右移

计算移动平均数时，因为$v_0=0$，故一开始曲线起点会比较低，不能很好的估测出一年中前两天的温度，
故需要进行偏差修正，让估测变得更好，更准确

也就是不用$v_t$，而是用$\frac{v_t}{(1-\beta ^t)}$
随着t的增加，$\beta ^t$将接近于0，所以当t很大时，修正偏差几乎没有作用，不过在开始阶段，修正偏差可一帮助你更好的预测。

# Momentum梯度下降法

其运行速度总是快于标准的梯度下降算法
基本思想是计算梯度的指数加权平均数，并利用该梯度更新你的权重

在每次迭代过程中，计算微分dw,db，之后用公式
![](/assets/images/170919_4.JPG)

这样就可以减缓梯度下降的幅度，因此你的算法走了一条更加直接的路径
在抵达最小值的路上减小了摆动

# RMSprop算法

全程是root mean squre prop算法，
我们需要用的新符号S_dw,S_db

接着RMSprop会这样更新参数值
![](/assets/images/170919_5.JPG)

我们希望S_dw相对较小，保持w方向上的变化
而又希望S_db相对较大，减缓b方向上的变化

RMSprop的影响就是更新后b方向上摆动较小，而w方向继续推进
以及可以用一个更大的学习率$\alpha$加快学习

# Adam优化算法

Adam本质上就是将Momentum和RMSprop结合在一起

使用Adam算法，首先要初始化
v_dw=0，S_dw=0，v_db=0，S_db=0
在迭代过程中
用mini-batch梯度下降法计算dw,db
接着用Momentum指数加权平均数计算
![](/assets/images/170919_6.JPG)
接着用RMSprop进行更新
![](/assets/images/170919_7.JPG)

一般使用Adam时需要计算偏差修正
![](/assets/images/170919_8.JPG)

最后更新权重
![](/assets/images/170919_9.JPG)

# 学习率衰减 

加快学习算法的一个方法就是随时间慢慢减少学习率，称之为学习率衰减

假设使用mini batch梯度下降时，迭代过程中会有噪音，朝向最小值方向下降，但可能不会精确的收敛，所以算法在最小点附近摆动，因为此时用的学习率是固定值

如果慢慢减少学习率的话，在初期，学习率还相对较大，学习速度也相对较快，但随着学习率降低，学习速度也会慢慢变小，所以算法会在最小值附近一小块区域内摆动
而不是在训练过程中，大幅度在最小值附近摆动

所以学习率衰减的本质在于，在学习初期，能承受较大步伐，但在开始收敛时，小的学习率能让步伐小一点

衰减学习率时，要遍历一次数据
可将学习率设为$\frac{1}{1+decay-rate*epoch-num}\alpha _0$
此处衰减率decay-rate是一个需要调整的超参数

# 局部最优

初期，人们担心优化算法会困在极差的局部最优，不过随着深度学习理论的不断发展，我们对局部最优的理解也发生了改变。
![](/assets/images/170919_10.JPG)
左图中，似乎各处都分布着局部最优，我们会认为梯度下降或者某个算法可能会困在一个局部最优中，而不会抵达全局最优

但这个理解并不正确，实际上，如果创建一个神经网络，通常梯度为零的点并不是图中的局部最优点，
实际上成本函数的零梯度点通常是鞍点，也就是有图上的蓝点

一个具有高维空间的函数，如果梯度为0，那么它在每个方向都可能是凸函数，也可能是凹函数，因此在高维空间中，更有可能碰到鞍点，而不是局部最优点

我们对低维度空间的大部分直觉，并不能应用到高维度空间中

所以
1. 我们的算法并不太可能困在极差的局部最优中，条件是你在训练较大的神经网络
2. 平稳段会使学习变得缓慢，可以使用优化算法加速学习
