---
layout: post
title: 笔记-吴恩达deeplearning之二
comments: true
date: 2017-09-13 20:29:10
tags:
- 笔记
- 深度学习
---
这门课讲的真清楚，轻轻松松就听得懂，  
但用markdown输数学公式，真是好麻烦呀。
<!--more-->
# 神经网络的表示

首先看看这个只有一个隐含层的神经网络。
![](/assets/images/170913_1.JPG)
如图，输入特征$x1,x2,x3$竖向堆叠起来，这是神经网络的输入层，包含神经网络的输入。另一层的圆圈，称之为隐含层。最后一层只有一个节点，这个只带一个节点的层叫输出层，负责输出预测值$\hat{y}$。

当使用神经网络时，训练集包含了输入x，还有目标输出y，隐含层的含义是，在训练集中，这些中间节点的真正数值不得而知。

通常用x来表示输入层，也可用$a^{[0]}$表示，[]代表层数，a表示激活，意味着网络中不同的值会传递给下一层，下一层隐藏层记为$a^{[1]}$，并将第一个节点表示为$a^{[1]}_1$，第二个节点表示为$a^{[1]}_2$，并依此类推。故这里$a^{[1]}$是一个四维向量，最后输出层会产生数值$a^{[2]}$的实数，即$\hat{y}$。

这个例子，即所谓的双层神经网络。当计算神经网络层数时不算入输入层，以隐藏层为第一层，输出层为第二层。在符号约定中，将输入层称为第零层。最后，隐藏层和输出层是带有参数的。

# 神经网络如何输出
![](/assets/images/170913_2.JPG)
例如之前的Logistic回归，圆圈代表回归计算的两个步骤，首先计算出z，然后计算激活函数，也就是sigmoid(z)函数，所以神经网络只不过多次重复计算这些步骤。
![](/assets/images/170913_3.JPG)
如图神经网络中有四个隐层单元，将其对等式进行整理，得四组等式。
可将其构成矩阵形式，并将$w^T$的堆叠称之为W，b的堆叠称之为b。
![](/assets/images/170913_4.JPG)
可得出$z^{[1]}=W^{[1]}x+b^{[1]}$，$a^{[1]}=\sigma (z^{[1]})$

故当你有个单隐层网络，需要计算四个等式：
$z^{[1]}=W^{[1]}x+b^{[1]}$
$a^{[1]}=\sigma (z^{[1]})$
$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$
$a^{[2]}=\sigma (z^{[2]})$

要搭建一个神经网络，你可以自己选择隐层里的激活函数，以及神经网络输出单元用什么激活函数，目前为止，我们一直采用$\sigma$激活函数，或许有时其它函数效果会更好。

当然最好选择非线性的激活函数，因为如果用线性激活函数，那么神经网络只是把输入线性组合再输出一次，除非你引入非线性激活函数。

# 神经网络的梯度下降法
### 参数：
对于单隐层神经网络，我们有维度是$(n^{[1]},n^{[0]})$的$W^{[1]}$，$(n^{[1]},1)$维向量的$b^{[1]}$，$(n^{[2]},n^{[2]})$维向量的$W^{[2]}$，$(n^{[2]},1)$维向量的$b^{[2]}$，以及$n^{[0]}$的多输入特征,$n^{[1]}$个隐藏单元，$n^{[2]}$个输出单元。
### 成本公式：
$J(W^{[1]},b^{[1]},W^{[2]},b^{[2]})=\frac{1}{m} \sum_{i=1}^{m} l(\hat{y},y)$
### 梯度下降(重复过程):
计算$\hat{y}^{(i)}，i=1……m$
$dW^{[1]}=\frac{\partial J}{\partial W^{[1]}}，db^{[1]}=\frac{\partial J}{\partial b^{[1]}}，……$
$W^{[1]} := W^{[1]}-\alpha dW^{[1]}$
$b^{[1]} := b^{[1]}-\alpha db^{[1]}$
$W^{[2]} := W^{[2]}-\alpha dW^{[2]}$
$b^{[2]} := b^{[2]}-\alpha db^{[2]}$

*强调，在训练神经网络时，随机初始化权重w非常重要，若将初始权重设为相等的数，梯度下降算法将完全无效。因为隐含单元将完全一样，意味着节点计算完全一样的函数，即不管梯度下降迭代多少次，将完全没有用处。*

# 神经网络的传播

深度神经网络每一层都有前向传播以及一个相对的反向传播步骤。
### 前向传播
输入$a^{[l-1]}$
输出$a^{[l]}$，缓存$z^{[l]}$，包括$w^{[l]},b^{[l]}$
$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$
$a^{[l]}=g^{[l]} (z^{[l]})$

### 反向传播
输入$da^{[l]}$
输出$da^{[l-1]},dW^{[L]},db^{[l]}$
$dz^{[l]}=da^{[l]}*g^{[l],}(z^[l])$
$dw^{[l]}=dz^{[l]}a^{[l-1]}$
$db^{[l]}=dz^{[l]}$
$da^{[l-1]}=w^{[l]T}dz^{[l]}$

整个传播过程如图所示：
![](/assets/images/170913_5.JPG)
前向传播时我们用输入数据x来初始化，反向传播时，若用Logistic回归时，则采用图中$da^{[l]}$这个公式，由相对于$\hat{y}$的损失函数的导数求得。
