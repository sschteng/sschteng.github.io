---
layout: post
title: 笔记-吴恩达deeplearning之一
comments: true
date: 2017-09-12 21:19:59
tags:
- 笔记
- 深度学习
---
最近在网易上听吴恩达的深度学习课程，据说这门课评价很高，是相当不错的深度学习启蒙课之一。  
吴恩达是斯坦福大学前教授，Google Brain项目发起人、领导者，众所周知的业界大佬。  
这门课靠无疑问的是全英文教学，不过几天前网易拿到了这门课的正版授权并且专门制作了中文版字幕，机会难得，所以听过之后，在此做做笔记，方便日后翻看。  
<!--more-->

深度学习通俗来说就是指规模很大的神经网络。

# 什么是神经网络？
![](\assets\images/170912_1.JPG)  
举个例子，例如房价的预测，我们想要寻找一个根据房屋面积，预测房价的函数，如图，那么用线性回归拟合曲线的话就是一条直线，当然房价不可能是一个负数，那么将直线下端弯曲一些，并使之结束于0，那么这条线就是我们想要的函数了。这几乎是最简单的神经网络了。  
![](\assets\images/170912_2.JPG)    
现将房屋面积作为神经网络的输入，称为x，通过小圆圈，输出价格，称为y，此处小圆圈就是一个独立神经元，这个网络实现了上图房价预测函数（被称为ReLU函数，即修正线性单元）的功能。这是一个很小的神经网络，大一点的神经网络就是把多个神经元堆叠起来。
![](\assets\images/170912_3.JPG)   
例如，此时还有房屋的其他特征，房屋面积和卧室数量可以估算家庭人数，邮编可以反映当地步行化程度，邮编和富裕程度可以评估学校质量，然后再考虑人们想要为此花费多少，最后帮你预测出房屋的价格。
  
神经网络的神奇之处在于，当实现它之后，只需要输入x，就能够得到y，不管你的训练集有多大，中间过程都将自己完成。小圆圈在神经网络中，被称为隐藏单元，每个输入都同时来自于四个特征。

值得一提的是，神经网络需要喂足够多的数据，即足够多的x,y训练样本。

# 为什么深度学习会兴起？
![](\assets\images/170912_4.JPG)  
如图，横轴代表完成任务的数据数量，纵轴代表机器学习算法的性能。

传统机器学习算法的表现，如支持向量机和logistic回归等，对映图中的红色曲线，性能在一开始，会随着数据的增加而上升，但一段时间之后，性能就会进入平台期，因为这些模型无法处理海量数据。在过去的几十年，我们遇到的很多问题，早期只能得到较少的数据量，而现在要收集海量数据轻而易举，远超传统学习算法能发挥作用的规模。
训练小号的神经网络，如黄色曲线，稍大一点的，如蓝色曲线，非常大的如绿色曲线，算法性能越来越好。  
  
注意，要想达到高性能水平的神经网络有两个条件：一是神经网络的训练规模要足够大，以发挥数据规模量巨大的优点，二是需要足够大的数据量。  

规模一直推动深度学习的进步，这里的规模不仅指神经网络的规模，还有“带标签的数据”量的规模。训练量不大时，算法效果取决于你手工设计的组件以及算法处理的一些细节，只有在大数据领域，才能见到神经网络。

# logistic回归

Logistic回归是一种学习算法，用在监督学习问题中，适用于输出标签是0或1的二元分类问题。

举个例子，输入特征向量$x$，可能是一张图片，你希望识别出这是不是一张猫图，输出一个预测值$\hat{y}$，$\hat{y}$是对$y$的预测，正式的说是一个概率值。当输入特征$x$满足条件时，$y$就是1，换句话，如果$x$是图片，你希望$\hat{y}$告诉你这张图片是猫的概率。

所以$x$是一个$n_x$维向量，logistic回归的参数是w，也是一个$n_x$维向量，b则是一个实数。那么已知输入$x$和参数w，b，如何计算预测$\hat{y}$呢？
可以用公式：$\hat{y} = \sigma(w^T+b)$

![](\assets\images/170912_5.JPG)
如图是sigmoid(z)的函数图形，横轴为z，是一条从0到1的光滑函数。

sigmoid(z)中z是实数，就是$\frac{1}{1+e^{-z}}$，当z非常大时，sigmoid(z)很接近1，z非常小时，sigmoid(z)很接近0。所以实现logistic回归时，要做的是学习参数w和b。训练w,b时需要定义一个成本函数。

再此之前，需要先定义下损失函数，损失函数衡量算法的运行情况，衡量预测输出值$\hat{y}$和$y$实际值有多接近。

常用的损失函数：$l(\hat{y},y) = -(ylog\hat{y}+(1-y)log(1-\hat{y}))$

例如$y$=1时，损失函数$l(\hat{y},y) = -log\hat{y}$，想要损失函数尽可能的小，那么$log\hat{y}$需要尽可能的大，即意味着$\hat{y}$要足够大，因为$\hat{y}$由sigmoid函数得出，永远不会比1大，那么也就是说$\hat{y}$将无限接近于1。  
另一种情况$y$=0时，损失函数$l(\hat{y},y） = -log(1-\hat{y})$，想要损失函数尽可能的小，$log(1-\hat{y})$需要尽可能的大，意味着$\hat{y}$要足够小，因为$\hat{y}$由sigmoid函数得出，永远不会比0小，那么也就是说$\hat{y}$无限接近于0。

损失函数是在单个训练样本中定义的，它衡量了神经网络在单个训练样本上的表现。下面定义一个成本函数，它衡量的是神经网络在全体训练样本上的表现。
$j(w,b)=\frac{1}{m} \sum_{i=1}^{m} l(\hat{y}^{(i)},y^{(i)})$

要习得合适的参数w和b，使得成本函数$J$(w,b)尽可能地小，可以利用梯度下降法。

## 梯度下降法

梯度下降法所做的就是从初始点开始，朝着最陡的下坡方向走，走一步即为梯度下降的一次迭代。
![](\assets\images/170912_6.JPG)
为了方便，先忽略上述b对函数$J$的影响，仅一维曲线代替多维曲线，梯度下降的做法是重复执行以下的更新操作，
w := w-$\alpha$dj(w)/dw,
$\alpha$代表学习率，控制每一次迭代或梯度下降法中的步长。

例如参数w，一开始很大，如图右侧，此时导数为正，新的w值等于w自身减去学习率乘导数，接着w会向左边走一步，像这样，让你的算法，渐渐地减小这个参数w。如果参数w一开始很小，如图左侧，此时倒数为负，新的w值会向右走一步，渐渐地算法会增大这个参数w。

当前只考虑参数w，那么在logistic回归中，成本函数含有参数w和b，同理
w:=w-$\alpha \partial j(w,b)/ \partial w$
b:=b-$\alpha \partial j(w,b)/ \partial b$

一个神经网络的计算都是按照前向及反向传播过程来实现的，首先计算出神经网络的输出，紧接着进行一个反向传输操作。后者用于计算对应的梯度或导数。

## logistic回归中的梯度下降

回顾公式：
$z=w^T+b$
$\hat{y} = a = \sigma（z）$
$l(a,y)=-(ylog(a)+（1-y）log(1-a))$

![](\assets\images/170912_7.JPG)  
假设样本只有两个特征$x_1,x_2$,为了计算z,我们需要参数w1,w2和b。
计算出损失函数$l$后，先向前一步，计算损失函数的导数da，即结果关于变量a的导数，再向前一步，计算出dz，$dz=a-y$，向前传播最后一步，计算出w,b需要如何变化，特别的，关于w1的导数$dw1=x_1dz$，同样的$dw2=x_2dz$，$db=dz$。

关于logistic回归的梯度下降法就是先计算出dz，再用dz计算出dw1,dw2和db,然后用梯度下降法的公式更新来参数w1,w2,和b。
$w1:=w1-\alpha d w1$
$w2:=w2-\alpha d w2$
$b:=b-\alpha db$

如上是将梯度下降法应用到logistic回归的单个训练样本上，现在我们要将他应用在m个训练样本上。
全局成本函数$J(w,b)$相当于从1到m项的损失函数求和后平均，它表明全局成本函数对w1的导数也同样是各项损失函数对w1导数的平均。
即$\frac{\partial}{\partial w1}J(w,b) = \frac{1}{m} \sum_{i=1}^{m} \frac{\partial}{\partial w1} l(a^{(i)},y^{(i)})$

同理可得$\partial$w2和$\partial$b。


