---
layout: post
title: 笔记-吴恩达deeplearning之五
comments: true
date: 2017-09-21 20:13:03
tags:
- 笔记
- 深度学习
---
神经网络的改变会涉及到许多不同的超参数设置，
对于超参数而言，系统地组织调试过程，
能更加方便有效的找到合适的超参数设定值。
<!--more-->
# 超参数的选取
关于训练深度最难的事情之一，是需要处理的参数的数量，
学习速率是需要调试的参数中最重要的参数，
还有一些参数，如Momentum参数、mini batch大小、隐藏单元，重要程度次一级，
其他因素的重要性排第三，如层数、学习衰减率等
![](/assets/images/170921_1.JPG)
在早一代的机器学习算法中，比如你有两个超参数，
常见的做法是在网格中取样点，然后系统的研究这些数值，
尝试过所有的参数后，选择出效果最好的一组参数。

在深度学习领用，我们常做的是随机选择点，
接着用这些随机选取的点试验超参数的效果，
之所以这么做，是因为很难提前预测哪个超参数更适合。

选择超参数时，另一个惯例是采用由粗糙到精细的策略，
先在大范围内进行取值，选择出大范围内最好的点，
或许该点周围还有效果更好的点，接下来，选中这块区域，
然后在其中更密集地取值或随机取值，通常更为有效。

关于搜索超参数的过程，大致有两种思想流派：

一种是时常照看一个模型，通常是有庞大的数据组，
但没有许多计算资源或足够的CPU和GPU的前提下，
即当它在试验时，你也可以定期逐渐改良超参数的设定，
直至达到想要的结果。

另一种是同时试验多种不同超参数设定的模型，
前提是拥有足够计算资源或CPU和GPU，
然后在训练结束，快速选择出工作效果最好的那个。

这两种方式的选择，主要取决于拥有的计算资源。

# Batch归一化

在深度学习兴起后，最重要的一个思想是它的一种算法，
叫Batch归一化，它会使你的参数搜索问题变得很容易。
使神经网络对超参数的选择更加稳定，范围更庞大，工作效果也很好。

当训练一个模型，比如Logistic回归时，归一化输入特征(输入样本减去平均数后再除以方差)可以加速学习过程，更易于算法优化。

那么对于更深的模型，没有输入特征值x，但有激活值$a^{[L]}$时，
如果想训练这些参数，比如$w^{3},b^{3}$，那归一化$a^{[2]}$的平均值和方差，
以便使$w^{3},b^{3}$的训练更有意义。

简单来说Batch归一化的作用就是，对于任何一个隐藏层而言，归一化a的值，能够以更快的速度来训练下一层的参数。
严格的来说，我们真正归一化的不是a，而是z。

### Batch归一化的使用
在神经网络中，已知一些中间值，假设有一些隐藏单元值，从$z^{(1)}$到$z^{(m)}$，
如下，首先计算平均值，接着计算方差，然后规范化$z^{(i)}$，
![](/assets/images/170921_2.JPG)
现在我们已经将这些z值标准化，使其含平均值0和标准单位方差，
所以z的每一个分量都含有平均值0和方差1，
或许我们需要隐藏单元有不同的分布，所以还需计算，
![](/assets/images/170921_3.JPG)
通过对$\gamma ,\beta$的合理设置，可以构造含其他平均值和方差的隐藏单元值。

Batch归一化的作用不止适用于输入层，同样适用于神经网络中的深度隐藏层。

假设有如上神经网络，每个节点进行两步计算，
如果没有进行Batch归一化，z将直接输入激活函数得到a，
否则，Batch归一化发生在z和a之间，

接下来可以任意选择需要的的优化算法。

使用中Batch归一化通常和训练集的mini-batch一起使用

用第一个mini-batch上计算出$z^{[1]}$，
接着Batch归一化，然后再正想传播求得$z^{[2]}$，
反向传播后，用梯度下降法更新参数，
这些都第一个mini-batch上进行的一步梯度下降法，

之后在第二个mini-batch上计算出$z^{[1]}$，
然后在此步中使数据Batch归一化，
如上依此类推，
也可尝试其他算法来对参数进行更新。

# Softmax回归

该回归能够对多个分类进行预测，而不是之前的两个分类，
关于Softmax激活函数，对于Softmax层而言，

首先计算一个临时变量t，$t=e^{z^{[L]}}$，就是对所有元素求幂，
接着对t归一化，求得$a^{[L]}=\frac{e^{z^{[L]}}}{\sum_{j=1}^{n} t_i}$
即$a^{[L]}$可以表示样本属于各分类的可能性

整个计算过程，从计算幂，到得出临时变量t，再归一化，
将此概括为一个Softmax激活函数。

![](/assets/images/170921_4.JPG)
例如一个神经网络输出层计算出z，有四个分类，即z为4*1维向量，
计算临时变量t，对元素进行幂运算。
最后，令输出层激活函数为Softmax激活函数，输出g，且总和为1。

对于四分类问题，损失函数一般如下，
$l(\hat{y},y)=- \sum_{j=1}^{4} j_ilog\hat{y}_j$

# TensorFlow

假设你有一个损失函数J需要最小话，$J(w)=w^2-20w+25$
使用TensorFlow将其最小化
![](/assets/images/170921_5.JPG)